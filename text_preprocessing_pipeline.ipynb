{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "stemer = PorterStemmer()\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "from emo_unicode import EMOTICONS\n",
        "from chat_words import SHORT_FORMS\n",
        "from unicode_emo import UNICODE_EMO\n",
        "\n",
        "from autocorrect import Speller\n",
        "speller = Speller()\n",
        "\n",
        "import contractions\n",
        "import wordninja\n",
        "\n",
        "\n",
        "# text preprocessing pipeline\n",
        "def text_preprocessing(\n",
        "    data,\n",
        "    lower_case = True,\n",
        "    expand_form = True,\n",
        "    separate_words = True,\n",
        "    punc = True,\n",
        "    stopwards = False,\n",
        "    stem = False,\n",
        "    lemmat = True,\n",
        "    urls = True,\n",
        "    htmltags = True,\n",
        "    remove_num_spaces = True,\n",
        "    spell_chec = True,\n",
        "    rm_emoj = False,\n",
        "    rm_emticon = False,\n",
        "    cn_emticon = True,\n",
        "    cn_emoj = True,\n",
        "    chat_con = True,\n",
        "):\n",
        "    # change to lowercase\n",
        "    if(lower_case):\n",
        "        data = data.lower()\n",
        "\n",
        "    # remove html tags\n",
        "    if(htmltags):\n",
        "        html_pattern = re.compile(\"<.*?>\")\n",
        "        data = html_pattern.sub(r'', data)\n",
        "\n",
        "    # remove urls\n",
        "    if(urls):\n",
        "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "        data = url_pattern.sub(r'', data)\n",
        "\n",
        "    # remove emojis\n",
        "    if(rm_emoj):\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "        data = emoji_pattern.sub(r'', data)\n",
        "\n",
        "    # remove emoticons\n",
        "    if(rm_emticon):\n",
        "        emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
        "        data = emoticon_pattern.sub(r'', data)\n",
        "\n",
        "    # convert emojis to words\n",
        "    if(cn_emoj):\n",
        "        for emot in UNICODE_EMO:\n",
        "            data = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()), data)\n",
        "\n",
        "    # convert emoticons to words\n",
        "    if(cn_emticon):\n",
        "        for emot in EMOTICONS:\n",
        "            data = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\", \"\").split()), data)\n",
        "\n",
        "    # contraction to expanded form\n",
        "    if(expand_form):\n",
        "        data = contractions.fix(data)\n",
        "\n",
        "    # chat words conversion\n",
        "    if(chat_con):\n",
        "        text = \" \"\n",
        "        for w in data.split():\n",
        "            if w in SHORT_FORMS.keys():\n",
        "                text = text + \" \" + SHORT_FORMS[w]\n",
        "            else:\n",
        "                text = text + \" \" + w\n",
        "        data = text\n",
        "\n",
        "    # spelling checker\n",
        "    if(spell_chec):\n",
        "        data = speller(data)\n",
        "\n",
        "    # seperate combined words\n",
        "    if(separate_words):\n",
        "      data = \" \".join(wordninja.split(data))\n",
        "\n",
        "    # remove stopwords\n",
        "    if(stopwards):\n",
        "        data = ' '.join([word for word in word_tokenize(data) if not word in list(stopwords.words('english'))])\n",
        "\n",
        "    # stemming\n",
        "    if(stem):\n",
        "        data = ' '.join([stemer.stem(word) for word in word_tokenize(data)])\n",
        "\n",
        "    # lemmatization\n",
        "    if(lemmat):\n",
        "        data = ' '.join([lemma.lemmatize(word) for word in word_tokenize(data)])\n",
        "\n",
        "    # remove punctuations\n",
        "    if(punc):\n",
        "        data = data.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # remove numbers and extra spaces\n",
        "    if(remove_num_spaces):\n",
        "        pattern = r\"\\d+\"\n",
        "        data = re.sub(pattern, \"\", str(data))\n",
        "        data.replace('\\s+', ' ')\n",
        "\n",
        "    return data\n",
        "\n"
      ],
      "metadata": {
        "id": "JqbiDwc-s55c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a365f52c-b10d-4c7b-8533-92c145b80ad4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.\n",
        "In their oldest forms, cakes were modifications of bread, but caakes now cover a wide range of preparations\n",
        "that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards,\n",
        "and pies BRB :-) ðŸ”¥. thisisatestsentence\"\"\"\n",
        "\n",
        "data_ = text_preprocessing(data)\n",
        "data_"
      ],
      "metadata": {
        "id": "SmIt17lhs52d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "a2b46d93-0b07-41de-8d4e-c4bfa78fc593"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cake is a form of sweet food made from flour sugar and other ingredient that is usually baked in their oldest form cake were modification of bread but cake now cover a wide range of preparation that can be simple or elaborate and that share feature with other dessert such a pastry meringue custard and pie be right back happy face smiley fire this is a test sentence'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['preprocessed_data'] = dataset['text'].apply(func = lambda x:text_preprocessing(x))\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "X8iL-rwTs5z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ewcLD1B8mjgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}